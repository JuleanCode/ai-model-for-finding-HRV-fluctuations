{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor, Button\n",
    "from scipy import signal\n",
    "import math\n",
    "import os\n",
    "\n",
    "directory  = 'data/Research_B/Data/'\n",
    "AllHeartbeats = []\n",
    "AllPeaks = []\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        \n",
    "        highest_peak_values = []\n",
    "        highest_peak_indices = []\n",
    "\n",
    "        data = []\n",
    "        RMSSD = []\n",
    "\n",
    "        with open(f, 'r') as file:\n",
    "            data_started = False\n",
    "            for line in file:\n",
    "                # Check if the line contains data\n",
    "                if data_started:\n",
    "                    values = line.strip().split()\n",
    "                    data.append([int(val) for val in values])\n",
    "                elif line.strip() == \"# EndOfHeader\":\n",
    "                    data_started = True\n",
    "\n",
    "            # Convert the data into a NumPy array\n",
    "            data = np.array(data)\n",
    "\n",
    "            heartbeat_data = data[:, 2]\n",
    "\n",
    "            #Toggle inverse heartbeatdata\n",
    "            if False:\n",
    "                heartbeat_data = -heartbeat_data\n",
    "\n",
    "            heightthreshold = 150  # Adjust this threshold as needed\n",
    "            widthtreshold = 200\n",
    "\n",
    "\n",
    "            #Butterworth filter\n",
    "            sos = signal.butter(2, 3, 'highpass',fs = 1000, output = 'sos')\n",
    "            filtered = signal.sosfilt(sos, heartbeat_data)\n",
    "\n",
    "            # Create an array for the x-axis (time)\n",
    "            time = np.arange(len(heartbeat_data))\n",
    "\n",
    "            peaks, _ = signal.find_peaks(filtered, height=heightthreshold, distance= widthtreshold)\n",
    "            AllHeartbeats.append(heartbeat_data)\n",
    "            AllPeaks.append(peaks)\n",
    "            print(count, end=' ')\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n",
      "[2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2980.425, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2773.8478260869565, 2819.288888888889, 2773.8478260869565, 2773.8478260869565, 2819.288888888889, 2819.288888888889, 2773.8478260869565, 2883.340909090909, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2819.288888888889, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2819.288888888889, 2883.340909090909, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2773.8478260869565, 2819.288888888889, 2773.8478260869565, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2819.288888888889, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2769.823529411765, 2769.823529411765, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2769.823529411765, 2769.823529411765, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2719.942857142857, 2751.1666666666665, 2751.1666666666665, 2751.1666666666665, 2701.135135135135, 2751.1666666666665, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2819.288888888889, 2883.340909090909, 2819.288888888889, 2819.288888888889, 2773.8478260869565, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2819.288888888889, 2883.340909090909, 2819.288888888889, 2883.340909090909, 2819.288888888889, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2980.425, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 3003.690476190476, 2908.9268292682927, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 2883.340909090909, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 2949.5581395348836, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 3003.690476190476, 3003.690476190476, 2949.5581395348836, 3003.690476190476, 3003.690476190476, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2908.9268292682927, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2908.9268292682927, 2908.9268292682927, 2980.425, 2980.425, 2908.9268292682927, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2701.135135135135, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2709.657894736842, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2709.657894736842, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2980.425, 2980.425, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2980.425, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513, 2979.2820512820513]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Plot de HRV data met de RMSSD formule\\n# Maak een figuur en as-object\\nfig, ax = plt.subplots(figsize=(12, 6))\\n\\n# Voeg de lijnplot toe aan de as\\nax.plot(HRVRMSSD, label=\\'HRV - RMSSD\\')\\n\\n# Stel de titel en labels in\\nax.set_xlabel(\\'Time (Row Index)\\')\\nax.set_ylabel(\\'Amplitude\\')\\nax.set_title(\\'HRV data - Nieuw RMSSD\\')\\n\\n# Voeg een raster en een legenda toe\\nax.grid(True)\\nax.legend()\\n\\nax.set_xlim(0, 1500)\\n\\nax.axvspan(180, 330, color=\"red\", alpha=0.3)\\nax.axvspan(330, 570, color=\"green\", alpha=0.3)\\nax.axvspan(570, 720, color=\"red\", alpha=0.3)\\nax.axvspan(720, 960, color=\"green\", alpha=0.3)\\nax.axvspan(960, 1110, color=\"red\", alpha=0.3)\\nax.axvspan(1110, 1350, color=\"green\", alpha=0.3)\\nax.legend()\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------- Nieuwe berekening -------------------------\n",
    "interval = 29000  # Meettijd per HRV waarde (ms)\n",
    "waarde1 = 0\n",
    "waarde2 = interval\n",
    "All_HRV = []\n",
    "\n",
    "def calculate_rmssd(heartbeat_data, peaks):\n",
    "    HRVRMSSD = []\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    \n",
    "    for i in range(int((len(heartbeat_data) - interval) / 1000)):\n",
    "        peaks_in_range_interval = [peak for peak in peaks if waarde1 <= peak <= waarde2]\n",
    "        RMSSD = [math.pow(heartbeat_data[peaks[j+1]] - heartbeat_data[peaks[j]], 2) for j, peak in enumerate(peaks_in_range_interval)]\n",
    "        HRVcount = sum(RMSSD)\n",
    "        HRVRMSSD.append(HRVcount / (len(peaks_in_range_interval) - 1))\n",
    "        waarde1 += 1000\n",
    "        waarde2 += 1000\n",
    "    \n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    return HRVRMSSD\n",
    "\n",
    "# Voer de functie uit om HRVRMSSD te berekenen\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVRMSSD = calculate_rmssd((AllHeartbeats[count]), peaks)\n",
    "    All_HRV.append(HRVRMSSD)\n",
    "    print(count, end=' ')\n",
    "    count += 1\n",
    "print('')\n",
    "print((All_HRV[0]))\n",
    "\n",
    "'''\n",
    "# Plot de HRV data met de RMSSD formule\n",
    "# Maak een figuur en as-object\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Voeg de lijnplot toe aan de as\n",
    "ax.plot(HRVRMSSD, label='HRV - RMSSD')\n",
    "\n",
    "# Stel de titel en labels in\n",
    "ax.set_xlabel('Time (Row Index)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('HRV data - Nieuw RMSSD')\n",
    "\n",
    "# Voeg een raster en een legenda toe\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlim(0, 1500)\n",
    "\n",
    "ax.axvspan(180, 330, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(330, 570, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(570, 720, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(720, 960, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(960, 1110, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(1110, 1350, color=\"green\", alpha=0.3)\n",
    "ax.legend()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mickm\\Documents\\ai-model-for-finding-HRV-fluctuations\\HRV_calculator.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m((All_HRV2[\u001b[39m500\u001b[39;49m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#plot de HRV data met de RMSSD formule\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Maak een figuur en as-object\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfig, ax = plt.subplots(figsize=(12, 6))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mplt.show()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Calculate HRV with RMSSD\n",
    "\n",
    "interval = 30000 #Meettijd per HRV waarde (ms)\n",
    "All_HRV2 = []\n",
    "def calculate_rmssd2(heartbeat_data, peaks):\n",
    "\n",
    "    HRVRMSSD2 = []\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    HRVcount = 0\n",
    "    q = 0\n",
    "    RMSSD2 = []\n",
    "    peaks_in_range_interval = 0\n",
    "    peaks_in_range_interval2 = len([peak for peak in peaks if waarde1 <= peak <= waarde2])\n",
    "\n",
    "    for i in range(int((len(heartbeat_data)-interval)/1000)):\n",
    "        #Neem de kwadraten van de verschillen tussen de waardes\n",
    "        for j in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            RMSSD2.append(math.pow(heartbeat_data[peaks[peaks_in_range_interval+q+1]] - heartbeat_data[peaks[peaks_in_range_interval+q]], 2)) #heartbeat_data vervangen door pieken\n",
    "            q = q+1\n",
    "\n",
    "        #Tel verkregen waardes bij elkaar op\n",
    "        HRVcount = sum(RMSSD2)\n",
    "\n",
    "        #Deel deze waarde door het totaal aantal waardes -1\n",
    "        HRVRMSSD2.append(HRVcount/(peaks_in_range_interval2-1))\n",
    "\n",
    "        #Reset variabelen\n",
    "        HRVcount = 0\n",
    "        q = 0\n",
    "        RMSSD2 = []\n",
    "        \n",
    "        #Verschuif het window met 1 seconden (1000 ms)\n",
    "        waarde1 = waarde1 + 1000\n",
    "        waarde2 = waarde2 + 1000\n",
    "        peaks_in_range_interval = len([peak for peak in peaks if 0 <= peak <= waarde1])\n",
    "        peaks_in_range_interval2 = len([peak for peak in peaks if 0 <= peak <= waarde2])\n",
    "    return HRVRMSSD2\n",
    "\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVRMSSD2 = calculate_rmssd2((AllHeartbeats[count]), peaks)\n",
    "    All_HRV2.append(HRVRMSSD2)\n",
    "    print(count, end=' ')\n",
    "    count += 1\n",
    "print('')\n",
    "print((All_HRV2[500]))\n",
    "    \n",
    "#plot de HRV data met de RMSSD formule\n",
    "# Maak een figuur en as-object\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Voeg de lijnplot toe aan de as\n",
    "ax.plot(HRVRMSSD, label='HRV - RMSSD')\n",
    "\n",
    "# Stel de titel en labels in\n",
    "ax.set_xlabel('Time (Row Index)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('HRV data - RMSSD')\n",
    "\n",
    "# Voeg een raster en een legenda toe\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlim(0, 1500)\n",
    "\n",
    "ax.axvspan(180, 330, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(330, 570, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(570, 720, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(720, 960, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(960, 1110, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(1110, 1350, color=\"green\", alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AllPeaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mickm\\Documents\\ai-model-for-finding-HRV-fluctuations\\HRV_calculator.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m HRVSDNN\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m peaks \u001b[39min\u001b[39;00m AllPeaks:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     HRVSDNN \u001b[39m=\u001b[39m calculate_sdnn((AllHeartbeats[count]), peaks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickm/Documents/ai-model-for-finding-HRV-fluctuations/HRV_calculator.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     All_HRV3\u001b[39m.\u001b[39mappend(HRVSDNN)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AllPeaks' is not defined"
     ]
    }
   ],
   "source": [
    "#Calculate HRV with SDNN\n",
    "\n",
    "All_HRV3 = []\n",
    "def calculate_sdnn(heartbeat_data, peaks):\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    peaks_in_range_interval = 0\n",
    "    peaks_in_range_interval2 = len([peak for peak in peaks if waarde1 <= peak <= waarde2])\n",
    "\n",
    "    SDNNGem = 0\n",
    "    SDNN = []\n",
    "    HRVSDNN = []\n",
    "    SDNNV = 0\n",
    "\n",
    "    r=0\n",
    "\n",
    "    for i in range(int((len(heartbeat_data)-interval)/1000)):\n",
    "        #Tel meetwaardes bij elkaar op en deel deze door het aantal waardes om het gemiddelde te berekenen.\n",
    "        for j in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNNGem += heartbeat_data[peaks[peaks_in_range_interval]]\n",
    "        SDNNGem = SDNNGem/(peaks_in_range_interval2 - peaks_in_range_interval)\n",
    "\n",
    "        #Bereken de afwijking per meetwaarde ten opzichte van het gemiddelde en neem hier het kwadraat van\n",
    "        for g in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNN.append(math.pow(heartbeat_data[peaks[peaks_in_range_interval+r]] - SDNNGem, 2)) #heartbeat_data vervangen door pieken\n",
    "            r = r+1\n",
    "\n",
    "        #Tel de gekwadrateerde afwijkingen bij elkaar op en deel deze door het aantal meet meetwaardes, en neem hier vervolgens de wortel van\n",
    "        SDNNV = sum(SDNN)\n",
    "        HRVSDNN.append(math.sqrt(SDNNV/len(SDNN)))\n",
    "\n",
    "        #Reset variabelen\n",
    "        r = 0\n",
    "        SDNNGem = 0\n",
    "        SDNNV = 0\n",
    "        SDNN = []\n",
    "\n",
    "        #Verschuif het window met 1 seconden (1000 ms)\n",
    "        waarde1 = waarde1 + 1000\n",
    "        waarde2 = waarde2 + 1000\n",
    "        peaks_in_range_interval = len([peak for peak in peaks if 0 <= peak <= waarde1])\n",
    "        peaks_in_range_interval2 = len([peak for peak in peaks if 0 <= peak <= waarde2])\n",
    "    return HRVSDNN\n",
    "\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVSDNN = calculate_sdnn((AllHeartbeats[count]), peaks)\n",
    "    All_HRV3.append(HRVSDNN)\n",
    "    print(count, end=' ')\n",
    "    count += 1\n",
    "print('')\n",
    "print((All_HRV3[0]))\n",
    "\n",
    "'''\n",
    "# Maak een figuur en as-object\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Voeg de lijnplot toe aan de as\n",
    "ax.plot(HRVSDNN, label='HRV - SDNN')\n",
    "\n",
    "# Stel de titel en labels in\n",
    "ax.set_xlabel('Time (Row Index)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('HRV data - SDNN')\n",
    "\n",
    "# Voeg een raster en een legenda toe\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlim(0, 1500)\n",
    "\n",
    "ax.axvspan(180, 330, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(330, 570, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(570, 720, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(720, 960, color=\"green\", alpha=0.3)\n",
    "ax.axvspan(960, 1110, color=\"red\", alpha=0.3)\n",
    "ax.axvspan(1110, 1350, color=\"green\", alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 115ms/step - loss: 1.2151 - val_loss: 0.9635\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2004 - val_loss: 0.9572\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1862 - val_loss: 0.9508\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1722 - val_loss: 0.9444\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1581 - val_loss: 0.9377\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1433 - val_loss: 0.9307\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1292 - val_loss: 0.9232\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1132 - val_loss: 0.9144\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0971 - val_loss: 0.9052\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0803 - val_loss: 0.8951\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0628 - val_loss: 0.8840\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0443 - val_loss: 0.8720\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0246 - val_loss: 0.8588\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0055 - val_loss: 0.8451\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.9846 - val_loss: 0.8307\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9636 - val_loss: 0.8156\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.9421 - val_loss: 0.7997\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9206 - val_loss: 0.7833\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8983 - val_loss: 0.7663\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8767 - val_loss: 0.7490\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8551 - val_loss: 0.7313\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8337 - val_loss: 0.7138\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8134 - val_loss: 0.6964\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7933 - val_loss: 0.6796\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7741 - val_loss: 0.6634\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7564 - val_loss: 0.6477\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7391 - val_loss: 0.6327\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7232 - val_loss: 0.6181\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7078 - val_loss: 0.6045\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6939 - val_loss: 0.5916\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6809 - val_loss: 0.5794\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6687 - val_loss: 0.5680\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6576 - val_loss: 0.5573\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6471 - val_loss: 0.5476\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6377 - val_loss: 0.5386\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6290 - val_loss: 0.5301\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6207 - val_loss: 0.5221\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6131 - val_loss: 0.5146\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6061 - val_loss: 0.5076\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5996 - val_loss: 0.5010\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5936 - val_loss: 0.4949\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5881 - val_loss: 0.4891\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5827 - val_loss: 0.4837\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5779 - val_loss: 0.4787\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5734 - val_loss: 0.4739\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5692 - val_loss: 0.4695\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5652 - val_loss: 0.4653\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5615 - val_loss: 0.4614\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5580 - val_loss: 0.4575\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5547 - val_loss: 0.4540\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5517 - val_loss: 0.4507\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5488 - val_loss: 0.4475\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5462 - val_loss: 0.4445\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5436 - val_loss: 0.4416\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5412 - val_loss: 0.4389\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5390 - val_loss: 0.4363\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5368 - val_loss: 0.4339\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5348 - val_loss: 0.4315\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5329 - val_loss: 0.4293\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5312 - val_loss: 0.4272\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5294 - val_loss: 0.4251\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5278 - val_loss: 0.4232\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5262 - val_loss: 0.4214\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5248 - val_loss: 0.4196\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5234 - val_loss: 0.4179\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5221 - val_loss: 0.4162\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5207 - val_loss: 0.4146\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5195 - val_loss: 0.4131\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5184 - val_loss: 0.4116\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5172 - val_loss: 0.4102\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5162 - val_loss: 0.4088\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5152 - val_loss: 0.4075\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5141 - val_loss: 0.4062\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5132 - val_loss: 0.4050\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5124 - val_loss: 0.4038\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5115 - val_loss: 0.4027\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5106 - val_loss: 0.4016\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5098 - val_loss: 0.4005\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5091 - val_loss: 0.3995\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5083 - val_loss: 0.3985\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5076 - val_loss: 0.3976\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5069 - val_loss: 0.3967\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5063 - val_loss: 0.3959\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5056 - val_loss: 0.3950\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5050 - val_loss: 0.3941\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5045 - val_loss: 0.3933\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5039 - val_loss: 0.3925\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5033 - val_loss: 0.3918\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5028 - val_loss: 0.3911\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5023 - val_loss: 0.3904\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5019 - val_loss: 0.3897\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5014 - val_loss: 0.3891\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5009 - val_loss: 0.3885\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5005 - val_loss: 0.3879\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5001 - val_loss: 0.3874\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4997 - val_loss: 0.3868\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4993 - val_loss: 0.3862\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4990 - val_loss: 0.3856\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4986 - val_loss: 0.3851\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4982 - val_loss: 0.3846\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4979 - val_loss: 0.3841\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4976 - val_loss: 0.3837\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4972 - val_loss: 0.3833\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4969 - val_loss: 0.3828\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4966 - val_loss: 0.3824\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4963 - val_loss: 0.3820\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4960 - val_loss: 0.3816\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4958 - val_loss: 0.3813\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4955 - val_loss: 0.3809\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4952 - val_loss: 0.3806\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4950 - val_loss: 0.3803\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4947 - val_loss: 0.3799\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4945 - val_loss: 0.3796\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4943 - val_loss: 0.3792\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4940 - val_loss: 0.3789\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4938 - val_loss: 0.3786\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4936 - val_loss: 0.3783\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4934 - val_loss: 0.3780\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4932 - val_loss: 0.3777\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4930 - val_loss: 0.3774\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4928 - val_loss: 0.3771\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4926 - val_loss: 0.3769\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4925 - val_loss: 0.3766\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4923 - val_loss: 0.3763\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4921 - val_loss: 0.3761\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4920 - val_loss: 0.3758\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4918 - val_loss: 0.3756\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4916 - val_loss: 0.3754\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4915 - val_loss: 0.3751\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4913 - val_loss: 0.3749\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4912 - val_loss: 0.3747\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4910 - val_loss: 0.3744\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4909 - val_loss: 0.3742\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4908 - val_loss: 0.3740\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4906 - val_loss: 0.3738\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4905 - val_loss: 0.3736\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4904 - val_loss: 0.3734\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4903 - val_loss: 0.3733\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4901 - val_loss: 0.3731\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4900 - val_loss: 0.3729\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4899 - val_loss: 0.3728\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4898 - val_loss: 0.3726\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4897 - val_loss: 0.3724\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4896 - val_loss: 0.3723\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4895 - val_loss: 0.3721\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4894 - val_loss: 0.3720\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4893 - val_loss: 0.3718\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4892 - val_loss: 0.3717\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4891 - val_loss: 0.3716\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4890 - val_loss: 0.3714\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4889 - val_loss: 0.3713\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4888 - val_loss: 0.3711\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4887 - val_loss: 0.3710\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4886 - val_loss: 0.3709\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4885 - val_loss: 0.3707\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4885 - val_loss: 0.3706\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4884 - val_loss: 0.3705\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4883 - val_loss: 0.3704\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4882 - val_loss: 0.3702\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4881 - val_loss: 0.3701\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4881 - val_loss: 0.3700\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4880 - val_loss: 0.3699\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4879 - val_loss: 0.3698\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4878 - val_loss: 0.3697\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4878 - val_loss: 0.3696\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4877 - val_loss: 0.3695\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4876 - val_loss: 0.3694\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4876 - val_loss: 0.3693\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4875 - val_loss: 0.3692\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4875 - val_loss: 0.3691\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4874 - val_loss: 0.3690\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4873 - val_loss: 0.3689\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4873 - val_loss: 0.3688\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4872 - val_loss: 0.3687\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4872 - val_loss: 0.3686\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4871 - val_loss: 0.3685\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4871 - val_loss: 0.3684\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4870 - val_loss: 0.3684\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4869 - val_loss: 0.3683\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4869 - val_loss: 0.3682\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4868 - val_loss: 0.3681\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4868 - val_loss: 0.3680\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4868 - val_loss: 0.3679\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4867 - val_loss: 0.3679\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4867 - val_loss: 0.3678\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4866 - val_loss: 0.3677\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4866 - val_loss: 0.3676\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4865 - val_loss: 0.3675\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4865 - val_loss: 0.3675\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4864 - val_loss: 0.3674\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4864 - val_loss: 0.3673\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4863 - val_loss: 0.3672\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4863 - val_loss: 0.3672\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4863 - val_loss: 0.3671\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4862 - val_loss: 0.3671\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4862 - val_loss: 0.3670\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4861 - val_loss: 0.3669\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4861 - val_loss: 0.3669\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4861 - val_loss: 0.3668\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4860 - val_loss: 0.3667\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4860 - val_loss: 0.3667\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4860 - val_loss: 0.3666\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4859 - val_loss: 0.3666\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4859 - val_loss: 0.3665\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4859 - val_loss: 0.3665\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4858 - val_loss: 0.3664\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4858 - val_loss: 0.3664\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4858 - val_loss: 0.3663\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4857 - val_loss: 0.3663\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4857 - val_loss: 0.3662\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4857 - val_loss: 0.3662\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4856 - val_loss: 0.3661\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4856 - val_loss: 0.3661\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4856 - val_loss: 0.3661\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4855 - val_loss: 0.3660\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4855 - val_loss: 0.3660\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4855 - val_loss: 0.3659\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4855 - val_loss: 0.3659\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4854 - val_loss: 0.3658\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4854 - val_loss: 0.3658\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4854 - val_loss: 0.3657\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4854 - val_loss: 0.3657\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4853 - val_loss: 0.3657\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4853 - val_loss: 0.3656\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4853 - val_loss: 0.3656\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4853 - val_loss: 0.3656\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4852 - val_loss: 0.3655\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4852 - val_loss: 0.3655\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4852 - val_loss: 0.3655\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4852 - val_loss: 0.3654\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4851 - val_loss: 0.3654\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4851 - val_loss: 0.3653\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4851 - val_loss: 0.3653\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4851 - val_loss: 0.3653\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4850 - val_loss: 0.3652\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4850 - val_loss: 0.3652\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4850 - val_loss: 0.3652\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4850 - val_loss: 0.3651\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4850 - val_loss: 0.3651\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4849 - val_loss: 0.3650\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4849 - val_loss: 0.3650\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4849 - val_loss: 0.3650\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4849 - val_loss: 0.3650\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4849 - val_loss: 0.3649\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4848 - val_loss: 0.3649\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4848 - val_loss: 0.3649\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4848 - val_loss: 0.3648\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4848 - val_loss: 0.3648\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4848 - val_loss: 0.3648\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4848 - val_loss: 0.3648\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4847 - val_loss: 0.3647\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.3647\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.3647\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.3646\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.3646\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4846 - val_loss: 0.3646\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.3646\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4846 - val_loss: 0.3645\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.3645\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.3645\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.3645\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.3644\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3644\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3644\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3644\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3643\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3643\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.3643\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3643\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3643\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3642\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4844 - val_loss: 0.3642\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4844 - val_loss: 0.3642\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3642\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3641\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4844 - val_loss: 0.3641\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4843 - val_loss: 0.3641\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3641\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4843 - val_loss: 0.3641\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3640\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3640\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3640\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3640\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4843 - val_loss: 0.3639\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.3639\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.3639\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4842 - val_loss: 0.3639\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4842 - val_loss: 0.3639\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.3638\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4842 - val_loss: 0.3638\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.3638\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.3638\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3638\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3638\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4841 - val_loss: 0.3637\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3636\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.3636\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3636\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3636\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3636\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3636\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3635\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.3634\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4839 - val_loss: 0.3634\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.3633\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3633\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3632\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3631\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3631\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3631\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.3631\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3631\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3631\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3631\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4837 - val_loss: 0.3631\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3631\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3630\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3629\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3629\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.3629\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4836 - val_loss: 0.3629\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4836 - val_loss: 0.3628\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3627\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.3627\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3627\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4835 - val_loss: 0.3626\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4834 - val_loss: 0.3626\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3625\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4834 - val_loss: 0.3624\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3624\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3623\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.3622\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3622\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3621\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3620\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4832 - val_loss: 0.3620\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4832 - val_loss: 0.3620\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.3620\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4616\n",
      "Loss: 0.4616\n",
      "3/3 [==============================] - 0s 1000us/step\n",
      "Mean Absolute Error (MAE): 0.462\n",
      "Mean Squared Error (MSE): 0.462\n",
      "R (R-squared): -0.858\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1850x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GRU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('DataTest.txt', sep='\\t')\n",
    "\n",
    "columns = ['SEX', 'AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['BMI', 'Y', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','AGE']]\n",
    "Y = data['SEX']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "# AI model\n",
    "model = Sequential()\n",
    "batch_size = 128\n",
    "epochs = 500\n",
    "\n",
    "model.add(LSTM(batch_size, input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[])\n",
    "\n",
    "train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "#print('Accuracy: {:0.3}'.format(100 * accuracy_score(Y_test, 1 * (y_pred_prob > 0.5))))\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n",
    "\n",
    "# Plot accuracy history\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 5.5)\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.legend(['loss', 'validation_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mickm\\AppData\\Local\\Temp\\ipykernel_27092\\590099423.py:10: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\lstm_hyperparameter_tuning\\tuner0.json\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 1s 133ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 37/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 38/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 39/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 40/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 41/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 42/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 43/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 44/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 45/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 46/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 47/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 48/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 49/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 50/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 51/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 52/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 53/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 54/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 55/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 56/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 57/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 58/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 59/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 60/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 61/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 62/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 63/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 64/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 65/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 66/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 67/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 68/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 69/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 70/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 71/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 72/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 73/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 74/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 75/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 76/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 77/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 78/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 79/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 80/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 81/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 82/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 83/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 84/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 85/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 86/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 87/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 88/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 89/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 90/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 91/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 92/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 93/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 94/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 95/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 96/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 97/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 98/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 99/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 100/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 101/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 102/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 103/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 104/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 105/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 106/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 107/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 108/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 109/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 110/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 111/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 112/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 113/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 114/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 115/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 116/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 117/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 118/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 119/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 120/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 121/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 122/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 123/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 124/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 125/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 126/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 127/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 128/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 129/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 130/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 131/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 132/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 133/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 134/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 135/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 136/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 137/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 138/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 139/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 140/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 141/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 142/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 143/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 144/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 145/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 146/150\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 147/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 148/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 149/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "Epoch 150/150\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4827 - val_loss: 0.3611\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4607\n",
      "Loss: 0.4607\n",
      "3/3 [==============================] - 0s 999us/step\n",
      "Mean Absolute Error (MAE): 0.461\n",
      "Mean Squared Error (MSE): 0.461\n",
      "R (R-squared): -0.854\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('DataTest.txt', sep='\\t')\n",
    "\n",
    "columns = ['SEX', 'AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['BMI', 'Y', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'AGE']]\n",
    "Y = data['SEX']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 128\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    batch_size = 128\n",
    "\n",
    "    model.add(LSTM(hp.Int('units', min_value=32, max_value=256, step=32), input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']), loss='mean_squared_error', metrics=[])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # You can adjust the number of trials\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_hyperparameter_tuning')\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Fit the best model to your data\n",
    "best_model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = best_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
