{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor, Button\n",
    "from scipy import signal\n",
    "import math\n",
    "import os\n",
    "\n",
    "directory  = 'data/Research_B/Data/'\n",
    "AllHeartbeats = []\n",
    "AllPeaks = []\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        \n",
    "        highest_peak_values = []\n",
    "        highest_peak_indices = []\n",
    "\n",
    "        data = []\n",
    "        RMSSD = []\n",
    "\n",
    "        with open(f, 'r') as file:\n",
    "            data_started = False\n",
    "            for line in file:\n",
    "                # Check if the line contains data\n",
    "                if data_started:\n",
    "                    values = line.strip().split()\n",
    "                    data.append([int(val) for val in values])\n",
    "                elif line.strip() == \"# EndOfHeader\":\n",
    "                    data_started = True\n",
    "\n",
    "            # Convert the data into a NumPy array\n",
    "            data = np.array(data)\n",
    "\n",
    "            heartbeat_data = data[:, 2]\n",
    "\n",
    "            #Toggle inverse heartbeatdata\n",
    "            if False:\n",
    "                heartbeat_data = -heartbeat_data\n",
    "\n",
    "            heightthreshold = 150  # Adjust this threshold as needed\n",
    "            widthtreshold = 200\n",
    "\n",
    "\n",
    "            #Butterworth filter\n",
    "            sos = signal.butter(2, 3, 'highpass',fs = 1000, output = 'sos')\n",
    "            filtered = signal.sosfilt(sos, heartbeat_data)\n",
    "\n",
    "            # Create an array for the x-axis (time)\n",
    "            time = np.arange(len(heartbeat_data))\n",
    "\n",
    "            peaks, _ = signal.find_peaks(filtered, height=heightthreshold, distance= widthtreshold)\n",
    "            AllHeartbeats.append(heartbeat_data)\n",
    "            AllPeaks.append(peaks)\n",
    "            print(count, end=' ')\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 53.8961965262856, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 52.51666402200353, 52.51666402200353, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 52.51666402200353, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072]\n"
     ]
    }
   ],
   "source": [
    "#Calculate HRV with RMSSD\n",
    "interval = 29000  # Meettijd per HRV waarde (ms)\n",
    "\n",
    "All_HRV = []\n",
    "\n",
    "def calculate_rmssd(heartbeat_data, peaks):\n",
    "    HRVRMSSD = []\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "\n",
    "    for i in range(int((len(heartbeat_data) - interval) / 1000)):\n",
    "        peaks_in_range_interval = [peak for peak in peaks if waarde1 <= peak <= waarde2]\n",
    "        squared_differences = [(heartbeat_data[peaks[i+1]] - heartbeat_data[peaks[i]]) ** 2 for i in range(len(peaks_in_range_interval)-1)]\n",
    "        mean_squared_diff = sum(squared_differences) / (len(peaks_in_range_interval) - 1)\n",
    "        rmssd_value = math.sqrt(mean_squared_diff)\n",
    "        HRVRMSSD.append(rmssd_value)\n",
    "        waarde1 += 1000\n",
    "        waarde2 += 1000\n",
    "\n",
    "    return HRVRMSSD\n",
    "\n",
    "# Voer de functie uit om HRVRMSSD te berekenen\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVRMSSD = calculate_rmssd(AllHeartbeats[count], peaks)\n",
    "    \n",
    "    # maakt een lijst binnen in de lijst\n",
    "    All_HRV.append(HRVRMSSD)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "def A():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n')# Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 1 0 0\\n\")  # Writing 1 for 'Stilte' column\n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 1 0\\n\")  # Writing 1 for 'Muziek' column\n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 0 1\\n\")  # Writing 1 for 'Tempo_Verlagende_Muziek' column\n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "\n",
    "def B():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "\n",
    "\n",
    "def C():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 1 0\\n\") \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "   \n",
    "\n",
    "def D():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "def E():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "def F():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif i <= 1350:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "with open('RMSSD.txt', 'w') as file:\n",
    "    file.write(\"HRV Stress Base Stilte Muziek TempoMuziek\\n\")  # Writing column headers\n",
    "    count = 0\n",
    "    for hrv_data in All_HRV:\n",
    "        for i, value in enumerate(hrv_data):\n",
    "            if (\n",
    "                (180 <= i <= 330) or\n",
    "                (570 <= i <= 720) or\n",
    "                (960 <= i <= 1110)\n",
    "            ):\n",
    "                file.write(f\"{value} 1 \")  # Writing 1 if within the specified ranges\n",
    "            elif i <= 1350:\n",
    "                file.write(f\"{value} 0 \")  # Writing 0 for other values\n",
    "            if count <6:\n",
    "                A()\n",
    "            elif count <12:\n",
    "                B()\n",
    "            elif count <19:\n",
    "                C()\n",
    "            elif count < 25:\n",
    "                D()\n",
    "            elif count < 30:\n",
    "                E()\n",
    "            elif count < 36:\n",
    "                F()\n",
    "        count += 1\n",
    "        file.write('\\n')  # Add a newline to separate different HRV data sets\n",
    "        \n",
    "\n",
    "\n",
    "print('')\n",
    "print(All_HRV[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 "
     ]
    }
   ],
   "source": [
    "#Calculate HRV with SDNN\n",
    "interval = 29000\n",
    "All_HRV3 = []\n",
    "def calculate_sdnn(heartbeat_data, peaks):\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    peaks_in_range_interval = 0\n",
    "    peaks_in_range_interval2 = len([peak for peak in peaks if waarde1 <= peak <= waarde2])\n",
    "\n",
    "    SDNNGem = 0\n",
    "    SDNN = []\n",
    "    HRVSDNN = []\n",
    "    SDNNV = 0\n",
    "\n",
    "    r=0\n",
    "\n",
    "    for i in range(int((len(heartbeat_data)-interval)/1000)):\n",
    "        #Tel meetwaardes bij elkaar op en deel deze door het aantal waardes om het gemiddelde te berekenen.\n",
    "        for j in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNNGem += heartbeat_data[peaks[peaks_in_range_interval]]\n",
    "        SDNNGem = SDNNGem/(peaks_in_range_interval2 - peaks_in_range_interval)\n",
    "\n",
    "        #Bereken de afwijking per meetwaarde ten opzichte van het gemiddelde en neem hier het kwadraat van\n",
    "        for g in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNN.append(math.pow(heartbeat_data[peaks[peaks_in_range_interval+r]] - SDNNGem, 2)) #heartbeat_data vervangen door pieken\n",
    "            r = r+1\n",
    "\n",
    "        #Tel de gekwadrateerde afwijkingen bij elkaar op en deel deze door het aantal meet meetwaardes, en neem hier vervolgens de wortel van\n",
    "        SDNNV = sum(SDNN)\n",
    "        HRVSDNN.append(math.sqrt(SDNNV/len(SDNN)))\n",
    "\n",
    "        #Reset variabelen\n",
    "        r = 0\n",
    "        SDNNGem = 0\n",
    "        SDNNV = 0\n",
    "        SDNN = []\n",
    "\n",
    "        #Verschuif het window met 1 seconden (1000 ms)\n",
    "        waarde1 = waarde1 + 1000\n",
    "        waarde2 = waarde2 + 1000\n",
    "        peaks_in_range_interval = len([peak for peak in peaks if 0 <= peak <= waarde1])\n",
    "        peaks_in_range_interval2 = len([peak for peak in peaks if 0 <= peak <= waarde2])\n",
    "    return HRVSDNN\n",
    "\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVSDNN = calculate_sdnn((AllHeartbeats[count]), peaks)\n",
    "    All_HRV3.append(HRVSDNN)\n",
    "    print(count, end=' ')\n",
    "    count += 1\n",
    "\n",
    "\n",
    "with open('SDNN.txt', 'w') as file:\n",
    "    file.write(\"HRV Stress Base Stilte Muziek TempoMuziek\\n\")  # Writing column headers\n",
    "    count = 0\n",
    "    for hrv_data in All_HRV3:\n",
    "        for i, value in enumerate(hrv_data):\n",
    "            if (\n",
    "                (180 <= i <= 330) or\n",
    "                (570 <= i <= 720) or\n",
    "                (960 <= i <= 1110)\n",
    "            ):\n",
    "                file.write(f\"{value} 1 \")  # Writing 1 if within the specified ranges\n",
    "            elif i <= 1350:\n",
    "                file.write(f\"{value} 0 \")  # Writing 0 for other values\n",
    "            if count <6:\n",
    "                A()\n",
    "            elif count <12:\n",
    "                B()\n",
    "            elif count <19:\n",
    "                C()\n",
    "            elif count < 25:\n",
    "                D()\n",
    "            elif count < 30:\n",
    "                E()\n",
    "            elif count < 36:\n",
    "                F()\n",
    "        count += 1\n",
    "        file.write('\\n')  # Add a newline to separate different HRV data sets\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "274/274 [==============================] - 2s 2ms/step - loss: 0.0698 - val_loss: 0.0071\n",
      "Epoch 2/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 3/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 4/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 5/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 6/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 7/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 8/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 9/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 10/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 11/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 12/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 13/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 14/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 15/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 16/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 17/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 18/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 19/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 20/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 21/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 22/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 23/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 24/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 25/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 26/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 27/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 28/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 29/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 30/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 31/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 32/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 33/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 34/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 35/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 36/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 37/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 38/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 39/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 40/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 41/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 42/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 43/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 44/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 45/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 46/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 47/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 48/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 49/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 50/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 51/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 52/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 53/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 54/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 55/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 56/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 57/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 58/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 59/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 60/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 61/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 62/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 63/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 64/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 65/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 66/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 67/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 68/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 69/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 70/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 71/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 72/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 73/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 74/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 75/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 76/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 77/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 78/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 79/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 80/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 81/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 82/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 83/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 84/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 85/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 86/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 87/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 88/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 89/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 90/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 91/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 92/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 93/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 94/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 95/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 96/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 97/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 98/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 99/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 100/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 101/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 102/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 103/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 104/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 105/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 106/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 107/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 108/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 109/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 110/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 111/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 112/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 113/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 114/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 115/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 116/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 117/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 118/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 119/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 120/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 121/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 122/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 123/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 124/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 125/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 126/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 127/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 128/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 129/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 130/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 131/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 132/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 133/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 134/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 135/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 136/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 137/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 138/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 139/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 140/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 141/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 142/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 143/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 144/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 145/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 146/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 147/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 148/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 149/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 150/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 151/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 152/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 153/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 154/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 155/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 156/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 157/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 158/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 159/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 160/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 161/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 162/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 164/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 165/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 166/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 167/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 168/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 169/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 170/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 171/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 172/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 173/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 174/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 175/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 176/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 177/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 178/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 179/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 180/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 181/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 182/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 183/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 184/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 185/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 186/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 187/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 188/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 189/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 190/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 191/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 192/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 193/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 194/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 195/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 196/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 197/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 198/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 199/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 200/200\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "76/76 [==============================] - 0s 847us/step - loss: 0.0027\n",
      "Loss: 0.0027\n",
      "304/304 [==============================] - 0s 634us/step\n",
      "Mean Absolute Error (MAE): 0.006\n",
      "Mean Squared Error (MSE): 0.003\n",
      "R (R-squared): 0.988\n",
      "Accuracy: 0.997\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1850x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GRU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('SDNN.txt', sep=' ')\n",
    "\n",
    "columns = ['HRV', 'Stress', 'Base', 'Stilte', 'Muziek', 'TempoMuziek']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['HRV', 'Base', 'Stilte', 'Muziek', 'TempoMuziek']]\n",
    "Y = data['Stress']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "# AI model\n",
    "model = Sequential()\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "model.add(LSTM(batch_size, input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[])\n",
    "\n",
    "train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "#print('Accuracy: {:0.3}'.format(100 * accuracy_score(Y_test, 1 * (y_pred_prob > 0.5))))\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n",
    "accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "print('Accuracy: {:0.3f}'.format(accuracy))\n",
    "\n",
    "# Plot accuracy history\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 5.5)\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.legend(['loss', 'validation_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 04m 23s]\n",
      "val_loss: 0.21380160748958588\n",
      "\n",
      "Best val_loss So Far: 0.21372322738170624\n",
      "Total elapsed time: 00h 24m 58s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |32                |units\n",
      "adam              |rmsprop           |optimizer\n",
      "\n",
      "Epoch 1/200\n",
      "1172/1172 [==============================] - 2s 1ms/step - loss: 0.2170 - val_loss: 0.2138\n",
      "Epoch 2/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 3/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 4/200\n",
      "1172/1172 [==============================] - 1s 1000us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 5/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2150 - val_loss: 0.2142\n",
      "Epoch 6/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2150 - val_loss: 0.2138\n",
      "Epoch 7/200\n",
      "1172/1172 [==============================] - 1s 994us/step - loss: 0.2150 - val_loss: 0.2143\n",
      "Epoch 8/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2150 - val_loss: 0.2141\n",
      "Epoch 9/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 10/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 11/200\n",
      "1172/1172 [==============================] - 1s 983us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 12/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2150 - val_loss: 0.2140\n",
      "Epoch 13/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 14/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 15/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 16/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 17/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 18/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 19/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 20/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 21/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 22/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 23/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 24/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2150 - val_loss: 0.2141\n",
      "Epoch 25/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 26/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 27/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 28/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 29/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 30/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 31/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 32/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 33/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 34/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 35/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 36/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 37/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 38/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 39/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 40/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 41/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 42/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 43/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 44/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 45/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 46/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 47/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 48/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 49/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 50/200\n",
      "1172/1172 [==============================] - 1s 998us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 51/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2143\n",
      "Epoch 52/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 53/200\n",
      "1172/1172 [==============================] - 1s 1000us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 54/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 55/200\n",
      "1172/1172 [==============================] - 1s 994us/step - loss: 0.2149 - val_loss: 0.2142\n",
      "Epoch 56/200\n",
      "1172/1172 [==============================] - 1s 983us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 57/200\n",
      "1172/1172 [==============================] - 1s 979us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 58/200\n",
      "1172/1172 [==============================] - 1s 985us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 59/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 60/200\n",
      "1172/1172 [==============================] - 1s 981us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 61/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 62/200\n",
      "1172/1172 [==============================] - 1s 991us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 63/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 64/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 65/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 66/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 67/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 68/200\n",
      "1172/1172 [==============================] - 1s 988us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 69/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 70/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 71/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 72/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 73/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2148 - val_loss: 0.2141\n",
      "Epoch 74/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 75/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 76/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 77/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 78/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 79/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 80/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 81/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 82/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2143\n",
      "Epoch 83/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 84/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 85/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 86/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 87/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 88/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 89/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 90/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 91/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 92/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2138\n",
      "Epoch 93/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 94/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 95/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 96/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 97/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 98/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 99/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 100/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 101/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 102/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 103/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 104/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 105/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 106/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 107/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 108/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 109/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 110/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 111/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 112/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 113/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 114/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 115/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 116/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 117/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 118/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 119/200\n",
      " 687/1172 [================>.............] - ETA: 0s - loss: 0.2150"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Calculation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import shutil\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('SDNN.txt', sep=' ')\n",
    "\n",
    "columns = ['HRV', 'Stress']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['HRV']]\n",
    "Y = data['Stress']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "if True:\n",
    "    if os.path.exists('my_dir'):\n",
    "        shutil.rmtree('my_dir')\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('units', min_value=32, max_value=256, step=32), input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']), loss='mean_squared_error', metrics=[])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # You can adjust the number of trials\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_hyperparameter_tuning')\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Fit the best model to your data\n",
    "best_model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = best_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = best_model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n",
    "accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "print('Accuracy: {:0.3f}'.format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
