{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor, Button\n",
    "from scipy import signal\n",
    "import math\n",
    "import os\n",
    "\n",
    "directory  = 'data/Research_B/Data/'\n",
    "AllHeartbeats = []\n",
    "AllPeaks = []\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        \n",
    "        highest_peak_values = []\n",
    "        highest_peak_indices = []\n",
    "\n",
    "        data = []\n",
    "        RMSSD = []\n",
    "\n",
    "        with open(f, 'r') as file:\n",
    "            data_started = False\n",
    "            for line in file:\n",
    "                # Check if the line contains data\n",
    "                if data_started:\n",
    "                    values = line.strip().split()\n",
    "                    data.append([int(val) for val in values])\n",
    "                elif line.strip() == \"# EndOfHeader\":\n",
    "                    data_started = True\n",
    "\n",
    "            # Convert the data into a NumPy array\n",
    "            data = np.array(data)\n",
    "\n",
    "            heartbeat_data = data[:, 2]\n",
    "\n",
    "            #Toggle inverse heartbeatdata\n",
    "            if False:\n",
    "                heartbeat_data = -heartbeat_data\n",
    "\n",
    "            heightthreshold = 150  # Adjust this threshold as needed\n",
    "            widthtreshold = 200\n",
    "\n",
    "\n",
    "            #Butterworth filter\n",
    "            sos = signal.butter(2, 3, 'highpass',fs = 1000, output = 'sos')\n",
    "            filtered = signal.sosfilt(sos, heartbeat_data)\n",
    "\n",
    "            # Create an array for the x-axis (time)\n",
    "            time = np.arange(len(heartbeat_data))\n",
    "\n",
    "            peaks, _ = signal.find_peaks(filtered, height=heightthreshold, distance= widthtreshold)\n",
    "            AllHeartbeats.append(heartbeat_data)\n",
    "            AllPeaks.append(peaks)\n",
    "            print(count, end=' ')\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 53.8961965262856, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 52.51666402200353, 52.51666402200353, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 52.51666402200353, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 50.989618320503126, 50.989618320503126, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.871820040227185, 51.423621895865026, 51.423621895865026, 51.423621895865026, 51.737904971218256, 51.423621895865026, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 54.16490754448239, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.09676700766881, 52.51666402200353, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.09676700766881, 53.68913043878367, 53.68913043878367, 53.68913043878367, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.28852284185279, 53.923387387265215, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.68913043878367, 54.16490754448239, 54.16490754448239, 54.16490754448239, 54.16490754448239, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.28852284185279, 53.28852284185279, 54.16490754448239, 53.28852284185279, 53.28852284185279, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.923387387265215, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.737904971218256, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.28403876040719, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.28403876040719, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 53.8961965262856, 53.8961965262856, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 53.8961965262856, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072, 51.38267691722072]\n"
     ]
    }
   ],
   "source": [
    "#Calculate HRV with RMSSD\n",
    "interval = 29000  # Meettijd per HRV waarde (ms)\n",
    "\n",
    "All_HRV = []\n",
    "\n",
    "def calculate_rmssd(heartbeat_data, peaks):\n",
    "    HRVRMSSD = []\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "\n",
    "    for i in range(int((len(heartbeat_data) - interval) / 1000)):\n",
    "        peaks_in_range_interval = [peak for peak in peaks if waarde1 <= peak <= waarde2]\n",
    "        squared_differences = [(heartbeat_data[peaks[i+1]] - heartbeat_data[peaks[i]]) ** 2 for i in range(len(peaks_in_range_interval)-1)]\n",
    "        mean_squared_diff = sum(squared_differences) / (len(peaks_in_range_interval) - 1)\n",
    "        rmssd_value = math.sqrt(mean_squared_diff)\n",
    "        HRVRMSSD.append(rmssd_value)\n",
    "        waarde1 += 1000\n",
    "        waarde2 += 1000\n",
    "\n",
    "    return HRVRMSSD\n",
    "\n",
    "# Voer de functie uit om HRVRMSSD te berekenen\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVRMSSD = calculate_rmssd(AllHeartbeats[count], peaks)\n",
    "    \n",
    "    # maakt een lijst binnen in de lijst\n",
    "    All_HRV.append(HRVRMSSD)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "def A():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n')# Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 1 0 0\\n\")  # Writing 1 for 'Stilte' column\n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 1 0\\n\")  # Writing 1 for 'Muziek' column\n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 0 1\\n\")  # Writing 1 for 'Tempo_Verlagende_Muziek' column\n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "\n",
    "def B():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "\n",
    "\n",
    "def C():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 1 0\\n\") \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "   \n",
    "\n",
    "def D():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "def E():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "def F():\n",
    "    if i < 180:\n",
    "        file.write(f'1 0 0 0\\n') # Writing 1 for 'Baseline'column\n",
    "    elif 330 <= i <= 570:\n",
    "        file.write(f\"0 0 0 1\\n\")  \n",
    "    elif 720 <= i <= 960:\n",
    "        file.write(f\"0 0 1 0\\n\")  \n",
    "    elif 1110 <= i <= 1350:\n",
    "        file.write(f\"0 1 0 0\\n\")  \n",
    "    else:\n",
    "        file.write(f\"0 0 0 0\\n\")  # Writing 0 for all other columns\n",
    "    \n",
    "\n",
    "with open('RMSSD.txt', 'w') as file:\n",
    "    file.write(\"HRV Stress Base Stilte Muziek TempoMuziek\\n\")  # Writing column headers\n",
    "    count = 0\n",
    "    for hrv_data in All_HRV:\n",
    "        for i, value in enumerate(hrv_data):\n",
    "            if (\n",
    "                (180 <= i <= 330) or\n",
    "                (570 <= i <= 720) or\n",
    "                (960 <= i <= 1110)\n",
    "            ):\n",
    "                file.write(f\"{value} 1 \")  # Writing 1 if within the specified ranges\n",
    "            else:\n",
    "                file.write(f\"{value} 0 \")  # Writing 0 for other values\n",
    "            if count <6:\n",
    "                A()\n",
    "            elif count <12:\n",
    "                B()\n",
    "            elif count <19:\n",
    "                C()\n",
    "            elif count < 25:\n",
    "                D()\n",
    "            elif count < 30:\n",
    "                E()\n",
    "            elif count < 36:\n",
    "                F()\n",
    "        count += 1\n",
    "        file.write('\\n')  # Add a newline to separate different HRV data sets\n",
    "        \n",
    "\n",
    "\n",
    "print('')\n",
    "print(All_HRV[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 "
     ]
    }
   ],
   "source": [
    "#Calculate HRV with SDNN\n",
    "interval = 29000\n",
    "All_HRV3 = []\n",
    "def calculate_sdnn(heartbeat_data, peaks):\n",
    "    waarde1 = 0\n",
    "    waarde2 = interval\n",
    "    peaks_in_range_interval = 0\n",
    "    peaks_in_range_interval2 = len([peak for peak in peaks if waarde1 <= peak <= waarde2])\n",
    "\n",
    "    SDNNGem = 0\n",
    "    SDNN = []\n",
    "    HRVSDNN = []\n",
    "    SDNNV = 0\n",
    "\n",
    "    r=0\n",
    "\n",
    "    for i in range(int((len(heartbeat_data)-interval)/1000)):\n",
    "        #Tel meetwaardes bij elkaar op en deel deze door het aantal waardes om het gemiddelde te berekenen.\n",
    "        for j in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNNGem += heartbeat_data[peaks[peaks_in_range_interval]]\n",
    "        SDNNGem = SDNNGem/(peaks_in_range_interval2 - peaks_in_range_interval)\n",
    "\n",
    "        #Bereken de afwijking per meetwaarde ten opzichte van het gemiddelde en neem hier het kwadraat van\n",
    "        for g in range(peaks_in_range_interval, peaks_in_range_interval2):\n",
    "            SDNN.append(math.pow(heartbeat_data[peaks[peaks_in_range_interval+r]] - SDNNGem, 2)) #heartbeat_data vervangen door pieken\n",
    "            r = r+1\n",
    "\n",
    "        #Tel de gekwadrateerde afwijkingen bij elkaar op en deel deze door het aantal meet meetwaardes, en neem hier vervolgens de wortel van\n",
    "        SDNNV = sum(SDNN)\n",
    "        HRVSDNN.append(math.sqrt(SDNNV/len(SDNN)))\n",
    "\n",
    "        #Reset variabelen\n",
    "        r = 0\n",
    "        SDNNGem = 0\n",
    "        SDNNV = 0\n",
    "        SDNN = []\n",
    "\n",
    "        #Verschuif het window met 1 seconden (1000 ms)\n",
    "        waarde1 = waarde1 + 1000\n",
    "        waarde2 = waarde2 + 1000\n",
    "        peaks_in_range_interval = len([peak for peak in peaks if 0 <= peak <= waarde1])\n",
    "        peaks_in_range_interval2 = len([peak for peak in peaks if 0 <= peak <= waarde2])\n",
    "    return HRVSDNN\n",
    "\n",
    "count = 0\n",
    "for peaks in AllPeaks:\n",
    "    HRVSDNN = calculate_sdnn((AllHeartbeats[count]), peaks)\n",
    "    All_HRV3.append(HRVSDNN)\n",
    "    print(count, end=' ')\n",
    "    count += 1\n",
    "\n",
    "\n",
    "\n",
    "with open('SDNN.txt', 'w') as file:\n",
    "    file.write(\"HRV Stress Base Stilte Muziek TempoMuziek\\n\")  # Writing column headers\n",
    "    count = 0\n",
    "    for hrv_data in All_HRV:\n",
    "        for i, value in enumerate(hrv_data):\n",
    "            if (\n",
    "                (180 <= i <= 330) or\n",
    "                (570 <= i <= 720) or\n",
    "                (960 <= i <= 1110)\n",
    "            ):\n",
    "                file.write(f\"{value} 1 \")  # Writing 1 if within the specified ranges\n",
    "            else:\n",
    "                file.write(f\"{value} 0 \")  # Writing 0 for other values\n",
    "            if count <6:\n",
    "                A()\n",
    "            elif count <12:\n",
    "                B()\n",
    "            elif count <19:\n",
    "                C()\n",
    "            elif count < 25:\n",
    "                D()\n",
    "            elif count < 30:\n",
    "                E()\n",
    "            elif count < 36:\n",
    "                F()\n",
    "        count += 1\n",
    "        file.write('\\n')  # Add a newline to separate different HRV data sets\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "293/293 [==============================] - 2s 3ms/step - loss: 0.0989 - val_loss: 0.0593\n",
      "Epoch 2/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0588\n",
      "Epoch 3/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0585\n",
      "Epoch 4/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 5/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 6/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 7/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 8/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 9/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 10/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 11/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 12/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 13/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 14/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 15/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 16/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 17/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 18/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 19/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0587\n",
      "Epoch 20/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 21/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 22/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 23/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 24/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 25/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 26/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 27/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 28/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 29/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 30/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 31/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 32/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 33/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 34/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 35/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 36/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 37/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 38/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 39/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 40/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 41/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 42/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 43/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 44/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 45/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 46/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 47/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 48/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 49/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 50/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 51/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 52/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 53/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 54/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 55/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 56/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 57/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 58/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 59/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 60/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 61/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0586\n",
      "Epoch 62/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 63/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 64/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 65/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 66/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 67/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 68/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 69/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 70/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 71/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 72/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 73/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 74/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 75/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 76/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 77/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 78/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 79/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 80/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 81/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 82/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 83/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 84/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 85/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 86/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 87/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 88/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 89/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 90/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 91/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 92/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 93/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 94/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 95/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 96/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 97/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 98/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 99/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 100/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 101/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 102/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 103/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 104/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 105/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 106/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 107/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 108/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 109/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 110/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 111/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 112/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 113/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 114/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 115/200\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 116/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 117/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 118/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 119/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 120/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 121/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 122/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 123/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 124/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 125/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 126/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 127/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 128/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 129/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 130/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 131/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 132/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 133/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 134/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 135/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 136/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 137/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 138/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 139/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 140/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 141/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 142/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 143/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 144/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 145/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 146/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 147/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 148/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 149/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 150/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 151/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 152/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 153/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 154/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 155/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 156/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 157/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 158/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 159/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 160/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 161/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 162/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 163/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 164/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 165/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 166/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 167/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 168/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 169/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 170/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 171/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 172/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 173/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 174/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 175/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 176/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 177/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 178/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 179/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 180/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 181/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 182/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 183/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 184/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 185/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 186/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0587\n",
      "Epoch 187/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 188/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 189/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 190/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 191/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0587\n",
      "Epoch 192/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 193/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 194/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 195/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 196/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 197/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 198/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "Epoch 199/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0586\n",
      "Epoch 200/200\n",
      "293/293 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0587\n",
      "82/82 [==============================] - 0s 826us/step - loss: 0.0587\n",
      "Loss: 0.0587\n",
      "326/326 [==============================] - 0s 644us/step\n",
      "Mean Absolute Error (MAE): 0.114\n",
      "Mean Squared Error (MSE): 0.059\n",
      "R (R-squared): 0.729\n",
      "Accuracy: 0.930\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1850x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GRU\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('RMSSD.txt', sep=' ')\n",
    "\n",
    "columns = ['HRV', 'Stress', 'Base', 'Stilte', 'Muziek', 'TempoMuziek']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['HRV', 'Base', 'Stilte', 'Muziek', 'TempoMuziek']]\n",
    "Y = data['Stress']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "# AI model\n",
    "model = Sequential()\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "model.add(LSTM(batch_size, input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[])\n",
    "\n",
    "train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "#print('Accuracy: {:0.3}'.format(100 * accuracy_score(Y_test, 1 * (y_pred_prob > 0.5))))\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n",
    "accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "print('Accuracy: {:0.3f}'.format(accuracy))\n",
    "\n",
    "# Plot accuracy history\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 5.5)\n",
    "loss = train_history.history['loss']\n",
    "validation_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.legend(['loss', 'validation_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 04m 23s]\n",
      "val_loss: 0.21380160748958588\n",
      "\n",
      "Best val_loss So Far: 0.21372322738170624\n",
      "Total elapsed time: 00h 24m 58s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |32                |units\n",
      "adam              |rmsprop           |optimizer\n",
      "\n",
      "Epoch 1/200\n",
      "1172/1172 [==============================] - 2s 1ms/step - loss: 0.2170 - val_loss: 0.2138\n",
      "Epoch 2/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 3/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 4/200\n",
      "1172/1172 [==============================] - 1s 1000us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 5/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2150 - val_loss: 0.2142\n",
      "Epoch 6/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2150 - val_loss: 0.2138\n",
      "Epoch 7/200\n",
      "1172/1172 [==============================] - 1s 994us/step - loss: 0.2150 - val_loss: 0.2143\n",
      "Epoch 8/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2150 - val_loss: 0.2141\n",
      "Epoch 9/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 10/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 11/200\n",
      "1172/1172 [==============================] - 1s 983us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 12/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2150 - val_loss: 0.2140\n",
      "Epoch 13/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 14/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 15/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 16/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 17/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 18/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2150 - val_loss: 0.2139\n",
      "Epoch 19/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 20/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 21/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 22/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 23/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 24/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2150 - val_loss: 0.2141\n",
      "Epoch 25/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 26/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 27/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 28/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 29/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 30/200\n",
      "1172/1172 [==============================] - 1s 995us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 31/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 32/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 33/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 34/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 35/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 36/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 37/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 38/200\n",
      "1172/1172 [==============================] - 1s 996us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 39/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 40/200\n",
      "1172/1172 [==============================] - 1s 999us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 41/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 42/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 43/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 44/200\n",
      "1172/1172 [==============================] - 1s 993us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 45/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 46/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 47/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 48/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 49/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 50/200\n",
      "1172/1172 [==============================] - 1s 998us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 51/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2143\n",
      "Epoch 52/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 53/200\n",
      "1172/1172 [==============================] - 1s 1000us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 54/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 55/200\n",
      "1172/1172 [==============================] - 1s 994us/step - loss: 0.2149 - val_loss: 0.2142\n",
      "Epoch 56/200\n",
      "1172/1172 [==============================] - 1s 983us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 57/200\n",
      "1172/1172 [==============================] - 1s 979us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 58/200\n",
      "1172/1172 [==============================] - 1s 985us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 59/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 60/200\n",
      "1172/1172 [==============================] - 1s 981us/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 61/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 62/200\n",
      "1172/1172 [==============================] - 1s 991us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 63/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 64/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 65/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 66/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 67/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 68/200\n",
      "1172/1172 [==============================] - 1s 988us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 69/200\n",
      "1172/1172 [==============================] - 1s 990us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 70/200\n",
      "1172/1172 [==============================] - 1s 992us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 71/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 72/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 73/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2148 - val_loss: 0.2141\n",
      "Epoch 74/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 75/200\n",
      "1172/1172 [==============================] - 1s 987us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 76/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 77/200\n",
      "1172/1172 [==============================] - 1s 986us/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 78/200\n",
      "1172/1172 [==============================] - 1s 984us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 79/200\n",
      "1172/1172 [==============================] - 1s 989us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 80/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 81/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 82/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2143\n",
      "Epoch 83/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 84/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 85/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 86/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 87/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 88/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 89/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 90/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 91/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 92/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2138\n",
      "Epoch 93/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 94/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 95/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 96/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 97/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 98/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2141\n",
      "Epoch 99/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 100/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 101/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 102/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 103/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 104/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 105/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 106/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 107/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2138\n",
      "Epoch 108/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 109/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 110/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 111/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 112/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 113/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 114/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2140\n",
      "Epoch 115/200\n",
      "1172/1172 [==============================] - 1s 997us/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 116/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2140\n",
      "Epoch 117/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2149 - val_loss: 0.2139\n",
      "Epoch 118/200\n",
      "1172/1172 [==============================] - 1s 1ms/step - loss: 0.2148 - val_loss: 0.2139\n",
      "Epoch 119/200\n",
      " 687/1172 [================>.............] - ETA: 0s - loss: 0.2150"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Calculation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import shutil\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('SDNN.txt', sep=' ')\n",
    "\n",
    "columns = ['HRV', 'Stress']\n",
    "data = data[columns]\n",
    "\n",
    "X = data[['HRV']]\n",
    "Y = data['Stress']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "if True:\n",
    "    if os.path.exists('my_dir'):\n",
    "        shutil.rmtree('my_dir')\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('units', min_value=32, max_value=256, step=32), input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']), loss='mean_squared_error', metrics=[])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # You can adjust the number of trials\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_hyperparameter_tuning')\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Fit the best model to your data\n",
    "best_model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "score = best_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "# Print training results\n",
    "print(\"Loss: {:0.4f}\".format(score))\n",
    "\n",
    "# Model voorspelt waarschijnlijkheden per klasse\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = best_model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "\n",
    "mae = mean_absolute_error(Y_test, y_pred_prob)\n",
    "print('Mean Absolute Error (MAE): {:0.3f}'.format(mae))\n",
    "mse = mean_squared_error(Y_test, y_pred_prob)\n",
    "print('Mean Squared Error (MSE): {:0.3f}'.format(mse))\n",
    "r_squared = r2_score(Y_test, y_pred_prob)\n",
    "print('R (R-squared): {:0.3f}'.format(r_squared))\n",
    "accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "print('Accuracy: {:0.3f}'.format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
